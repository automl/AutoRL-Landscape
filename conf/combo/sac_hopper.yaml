# @package _global_

# defaults:
#   - agent: sac
#   - env: hopper

agent:
  name: SAC
  hps:
    policy: 'MlpPolicy'
    use_sde: False
    learning_starts: 10000
env:
  name: Hopper-v3
  total_timesteps: 1000000
eval:
  freq_eval_inteval: 1000
  freq_eval_episodes: 5
# from sac.yaml:
# name: SAC
# hps:
#   n_step: 1
#   beta: 0.2
#   replay_capacity: 100000 # from CARL, original uses 1000000
#   warmup_steps: 100 # for filling the replay buffer. warmup_steps >= batch_size
#   exploration_steps: 0 # during this time, use a random (uniform) policy but already learn the true policy.
#   batch_size: 32 # 256 # from repro
#   target_smoothing: 0.005 # Ï„ for ema of Q
